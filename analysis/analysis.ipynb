{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16FlYMaze : Analysis of Gr64f/CsChrimson flies with different Optogenetic Pulse Structures ($\\lambda$ = 625nm)\n",
    "\n",
    "**Data Source:** *16FlYMaze System, Turner Lab, HHMI Janelia Research Campus*\n",
    "\n",
    "**Collected By:** *Kaitlyn Boone, Aparna Dev, PTR, HHMI Janelia Research Campus*\n",
    "\n",
    "**Code Author:** *Rishika Mohanta*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np                      # for numerical operations\n",
    "import pandas as pd                     # for data manipulation\n",
    "import matplotlib.pyplot as plt         # for plotting\n",
    "import scipy.optimize as opt            # for optimization\n",
    "from joblib import Parallel, delayed    # for parallelization\n",
    "from tqdm.notebook import tqdm          # for progress bar\n",
    "import pickle                           # for saving and loading\n",
    "import os                               # for file operations\n",
    "import datetime                         # for datetime operations\n",
    "import flyoptostim.rdp_client as rdp    # for connecting RDP security standards\n",
    "from joblib import Parallel, delayed    # for parallelization\n",
    "from tqdm.notebook import tqdm          # for progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Variables (Change if Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/pulStr_20-07-2023.ezip' # path to encrypted files\n",
    "save_path = '../processed_data/pulStr_20-07-2023' # path to save processed data\n",
    "quality_control = 'full' # whether to perform quality control (valid options: minimal, full, none)\n",
    "last_date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") # get current date and time\n",
    "n_jobs = 10 # number of jobs for parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decrypt and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unencrypt Data\n",
    "rdp.unlock_and_unzip_file(data_path,key_dir='../key.key')\n",
    "\n",
    "# Redefine data path\n",
    "data_path = data_path[:-5]+'/'\n",
    "\n",
    "# load data\n",
    "choice_data = np.loadtxt(data_path + 'choices.csv', delimiter=',')\n",
    "reward_data = np.loadtxt(data_path + 'rewards.csv', delimiter=',')\n",
    "\n",
    "# turn into integers\n",
    "choice_data = choice_data.astype(int)\n",
    "reward_data = reward_data.astype(int)\n",
    "\n",
    "if choice_data.shape != reward_data.shape:\n",
    "    raise ValueError('Sizes do not match.')\n",
    "\n",
    "N = choice_data.shape[0]    # number of flies\n",
    "\n",
    "print(\"Data loaded successfully with N = {} flies and {} maximum trials\".format(N, choice_data.shape[1]))\n",
    "\n",
    "# metadata\n",
    "metadata = pd.read_csv(data_path + 'metadata.csv')\n",
    "metadata['Experiment Start Time'] = pd.to_datetime(metadata['Experiment Start Time'], format='%Y-%m-%d %H:%M:%S')\n",
    "metadata['Starvation Time'] = pd.to_datetime(metadata['Starvation Time'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUALITY CONTROL\n",
    "\n",
    "if quality_control == 'minimal':\n",
    "    qc = np.loadtxt(data_path + 'quality_control.csv', delimiter=',').astype(bool)\n",
    "    choice_data = choice_data[qc]\n",
    "    reward_data = reward_data[qc]\n",
    "    metadata = metadata[qc]\n",
    "    metadata.reset_index(drop=True, inplace=True)\n",
    "if quality_control == 'full':\n",
    "    qc = np.loadtxt(data_path + 'quality_control.csv', delimiter=',').astype(bool)\n",
    "    metadata = metadata[qc]\n",
    "    metadata = metadata[metadata['Experiment Start Time'] < last_date].groupby('Fly Experiment').head(10)\n",
    "    choice_data = choice_data[metadata.index]\n",
    "    reward_data = reward_data[metadata.index]\n",
    "    metadata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"{}/{} ({}) flies passed quality control\".format(choice_data.shape[0], N, \"{:0.2f}\".format(choice_data.shape[0]/N*100)))\n",
    "\n",
    "N = choice_data.shape[0]    # number of flies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cumulative Choices made by flies over trials\n",
    "\n",
    "The Cumulative Choice Plot (Choice 1 vs Choice 2) is shown below for the different experiments. The Reward Rate is also shown in the same plot. The Reward Rate is calculated as the number of times the fly chose the rewarded arm divided by the total number of times the fly chose either arm. The Reward Rate is calculated for every 10 trials and is plotted at the end of the 10 trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CHOICES \n",
    "\n",
    "order_map = {\n",
    "    'DoubleT_Learning.csv':5, \n",
    "    'DoubleT_Learning_reciprocal.csv':5,\n",
    "    'HalfT_Learning.csv':3, \n",
    "    'HalfT_Learning_reciprocal.csv':3,\n",
    "    'HighI_Learning.csv':2,\n",
    "    'HighI_Learning_reciprocal.csv':2,\n",
    "    'LowI_Learning.csv':0,\n",
    "    'LowI_Learning_reciprocal.csv':0,\n",
    "    'SplitT_Learning.csv':4,\n",
    "    'SplitT_Learning_reciprocal.csv':4,\n",
    "    'Standard_Learning.csv':1,\n",
    "    'Standard_Learning_reciprocal.csv':1\n",
    "}\n",
    "cmap = 'viridis'\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(9, 6))\n",
    "for exp in metadata['Fly Experiment'].sort_values().unique():\n",
    "    choice_set = choice_data[metadata[metadata['Fly Experiment'] == exp].index]\n",
    "    reward_set = reward_data[metadata[metadata['Fly Experiment'] == exp].index]\n",
    "    N_set = choice_set.shape[0]\n",
    "    for i in range(N_set):\n",
    "        reward_rate = np.convolve(reward_set[i,:], np.ones(10)/10, mode='same')\n",
    "        if \"_reciprocal\" in exp:\n",
    "            ax.flatten()[order_map[exp]].scatter(np.cumsum(choice_set[i,:]==1), np.cumsum(choice_set[i,:]==0), c=reward_rate, cmap=cmap, s=2, vmin=0, vmax=1)\n",
    "        else:\n",
    "            ax.flatten()[order_map[exp]].scatter(np.cumsum(choice_set[i,:]==0), np.cumsum(choice_set[i,:]==1), c=reward_rate, cmap=cmap, s=2, vmin=0, vmax=1)\n",
    "    ax.flatten()[order_map[exp]].plot([0, choice_set.shape[1]], [0, choice_set.shape[1]], 'k--')\n",
    "    ax.flatten()[order_map[exp]].set_xlabel('Left choices')\n",
    "    ax.flatten()[order_map[exp]].set_ylabel('Right choices')\n",
    "    ax.flatten()[order_map[exp]].set_xlim([0, choice_set.shape[1]])\n",
    "    ax.flatten()[order_map[exp]].set_ylim([0, choice_set.shape[1]])\n",
    "    ax.flatten()[order_map[exp]].set_title(exp[:-4].split('_reciprocal')[0])\n",
    "    ax.flatten()[order_map[exp]].set_aspect('equal')\n",
    "    ax.flatten()[order_map[exp]].spines['top'].set_visible(False)\n",
    "    ax.flatten()[order_map[exp]].spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# add a colorbar to the entire figure\n",
    "cbar_ax = fig.add_axes([1.0, 0.15, 0.02, 0.7])\n",
    "cbar = fig.colorbar(ax.flatten()[1].collections[0], cax=cbar_ax, orientation='vertical')\n",
    "cbar.set_label('Reward rate (last 10 trials)')\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Multi-Factor Q-Learning Regression Model\n",
    "\n",
    "In order to assess the strength of each reward pulse, fit a reinforcement learning model to the data. The best known cognitive model of the data so far is the Q-Learning model with (1) Forgetting, (2) Omission-sensitivity and (3) Softmax Policy Function. \n",
    "\n",
    "Q-Learning is a reinforcement learning model that learns the value of each action (choice). Since each trial is identical to the last, this is a single-state model (1-state 2-action Markov Decision Process). In our task, the reward is a flash of light but the strength of the reward depends on the structure of the pulse. The value of each action is updated based on the reward received after the action is taken.\n",
    "\n",
    "The value of the chosen action is updated using the following equation in experiments with the $i_{th}$ pulse structure:\n",
    "\n",
    "$$Q_{t+1}(a_t) = Q_t(a_t) + \\alpha_{learn}*(r_t*kappa_i + (1-r_t)*kappa_{omission} - Q_t(a_t))$$\n",
    "\n",
    "where $Q_{t+1}(a_t)$ is the updated value of the chosen action, $Q_t(a_t)$ is the previous value of the chosen action, $\\alpha_{learn}$ is the learning rate, $r_t$ is whether a reward received after the action is taken, $kappa_i$ is the reward strength of the $i_{th}$ pulse structure and $kappa_{omission}$ amount by with the value of the chosen action is increased/decreased when no reward is received.\n",
    "\n",
    "The value of the unchosen action is updated using the following equation in experiments with the $i_{th}$ pulse structure:\n",
    "\n",
    "$$Q_{t+1}(a_{t+1}) = (1-\\alpha_{forget})*Q_t(a_{t+1})$$\n",
    "\n",
    "where $Q_{t+1}(a_{t+1})$ is the updated value of the unchosen action, $Q_t(a_{t+1})$ is the previous value of the unchosen action, $\\alpha_{forget}$ is the forgetting rate, representing the slow decay of the value of the unchosen action over time.\n",
    "\n",
    "Since, we can assume that all parameters except the reward strength of the pulse structure are the same for all experiments, we can fit a single model to all the data using a bernoulli regression model. \n",
    "\n",
    "The probability of choosing the second odor ($C_t = 1$) is given by the softmax policy function:\n",
    "\n",
    "$$P(a_t = 1) = softmax(Q_t)_1 = \\frac{e^{Q_t(a_t = 1)}}{e^{Q_t(a_t = 0)} + e^{Q_t(a_t = 1)}}$$\n",
    "\n",
    "where $P(a_t = 0)$ is the probability of choosing the first odor, $Q_t(a_t = 0)$ is the value of the first odor and $Q_t(a_t = 1)$ is the value of the second odor.\n",
    "\n",
    "We fit the model my minimizing the negative log likelihood of the observed choices ($C_{1:t}$) given the model parameters ($\\theta=[\\alpha_{learn}, \\alpha_{forget}, \\kappa_{omission}, \\kappa_1, \\kappa_2, ..., \\kappa_n]$) where $n$ is the number of pulse structures used in the experiment. The likelihood of the observed choices is given by:\n",
    "\n",
    "$$C_{1:t} \\sim Bernoulli(P(a_t = 1)_{1:t})$$\n",
    "$$P(a_t = 1)_{1:t} = \\frac{e^{Q_t(a_t = 1)_{1:t}}}{e^{Q_t(a_t = 0)_{1:t}} + e^{Q_t(a_t = 1)_{1:t}}}$$\n",
    "\n",
    "\n",
    "where $C_{1:t}$ is the vector of choices made by the fly over trials and $P(a_t = 1)_{1:t}$ is the vector of probabilities of choosing the second odor over trials. The Q-values are updated using the equations above. Starting with $Q_0(a_t = 0) = 0$ and $Q_0(a_t = 1) = 0$.\n",
    "\n",
    "**This allowes us to identify the relative strength of each pulse structure.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT MULTIFACTOR OSFQL-MODEL\n",
    "\n",
    "def fit_ql(params,c,r,e,return_probs=False):\n",
    "    \"\"\"\n",
    "    A function return the probability of choosing odor 2 in a two-odor choice task.\n",
    "    The choices are made according to a Q-learning rule with forgetting and omission-sensitivity and a softmax policy.\n",
    "    \n",
    "    Args:\n",
    "        params (list): list of parameters (alpha_learn, alpha_forget, kappa_omission, kappa_1, kappa_2, ..., kappa_n)\n",
    "        c (list): matrix of choices (1 for odor 2, 0 for odor 1) (n_flies x n_trials)\n",
    "        r (list): matrix of rewards (1 for reward, 0 for no reward) (n_flies x n_trials)\n",
    "        e (list): matrix of pulse structure identifiers (n_flies x n_trials)\n",
    "        return_probs (bool): whether to return the probability of choosing odor 2 for each trial (default: False)\n",
    "\n",
    "    Returns:\n",
    "        float: negative log-likelihood of the data given the parameters\n",
    "        np.array: probability of choosing odor 2 for each trial (only if return_probs=True) (n_flies x n_trials)\n",
    "    \"\"\"\n",
    "    liks = [] # list of log-likelihoods\n",
    "    alpha_learn, alpha_forget, kappa_omission, kappa = params[0], params[1], params[2], params[3:] # unpack parameters\n",
    "    \n",
    "    if return_probs: ps = [] # list of probabilities per fly (only if return_probs=True)\n",
    "    \n",
    "    for i in range(len(c)):\n",
    "        # remove nan values\n",
    "        ct = c[i][~np.isnan(c[i])].copy()\n",
    "        rt = r[i][~np.isnan(r[i])].copy()\n",
    "        # initialize q-values\n",
    "        q = np.zeros(2)\n",
    "\n",
    "        if return_probs: p = [] # list of probabilities per trial (only if return_probs=True)\n",
    "\n",
    "        for j in range(len(ct)): # loop over trials\n",
    "            # use softmax policy\n",
    "            pval = np.clip(np.exp(q[1])/np.sum(np.exp(q)),1e-6,1-1e-6) # probability of choosing odor 2 (clipped to avoid numerical errors)\n",
    "            \n",
    "            if return_probs: p.append(pval) # append probability (only if return_probs=True)\n",
    "\n",
    "            liks.append(np.log(pval)*ct[j] + np.log(1-pval)*(1-ct[j])) # append log-likelihood using Bernoulli likelihood\n",
    "\n",
    "            # QLearning Rule\n",
    "            q[ct[j]] = q[ct[j]] + alpha_learn*((rt[j]*kappa[e[i]]+(1-rt[j])*kappa_omission)-q[ct[j]])\n",
    "            q[1-ct[j]] = (1-alpha_forget)*q[1-ct[j]]\n",
    "\n",
    "        if return_probs: \n",
    "            temp = np.nan*np.zeros(len(c[i])) # initialize array of probabilities\n",
    "            temp[~np.isnan(c[i])] = p # fill in array of probabilities\n",
    "            ps.append(temp) # append array of probabilities (only if return_probs=True)\n",
    "\n",
    "    return -np.sum(liks) if not return_probs else (-np.sum(liks), np.array(ps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model Parameters and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a map from unique pulse structure to index\n",
    "exp_mapper = {exp:i for i, exp in enumerate(metadata['Fly Experiment'].apply(lambda x: x.split('.')[0].split('_reciprocal')[0]).unique())}\n",
    "# make a map from index to unique pulse structure\n",
    "exp_mapper_inv = {i:exp.split('_')[0] for i, exp in enumerate(metadata['Fly Experiment'].apply(lambda x: x.split('.')[0].split('_reciprocal')[0]).unique())}\n",
    "\n",
    "r = reward_data.copy() # rewards\n",
    "c = choice_data.copy() # choices\n",
    "e = metadata['Fly Experiment'].apply(lambda x: exp_mapper[x.split('.')[0].split('_reciprocal')[0]]).values # map pulse structure to index\n",
    "\n",
    "num_e = np.max(e)+1 # number of unique pulse structures\n",
    "init_params = np.array([0.5,0.5,1]+[1]*num_e) # initial parameters\n",
    "bounds = [(0,1),(0,1),(0,5)]+[(0,5)]*num_e # bounds for parameters\n",
    "\n",
    "# create the save directory if it doesn't exist\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization techniques\n",
    "\n",
    "There are many ways to optimize the data. First, we try to find the best parameters using global optimization techniques. Then, we use the best parameters to initialize the optimization process for multiple bootstrap samples of the data to estimate the uncertainty in the parameters.\n",
    "\n",
    "#### Global Optimization\n",
    "\n",
    "We use differential evolution to find the best parameters for the model. Differential evolution is a global optimization technique that uses a population of candidate solutions to find the best parameters. The algorithm works by creating a population of candidate solutions and then iteratively updating the population by replacing the worst candidate solutions with new candidate solutions. \n",
    "\n",
    "We use a population of 100 candidate solutions and run the algorithm for a maximum of 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL OPTIMIZATION USING DE from SciPy\n",
    "\n",
    "go_results = opt.differential_evolution(\n",
    "    lambda x: fit_ql(x,c,r,e), # objective function\n",
    "    bounds, # bounds for parameters\n",
    "    disp=True, # display progress\n",
    "    maxiter=1000, # maximum number of iterations\n",
    "    popsize=100 # population size\n",
    ")\n",
    "\n",
    "# save results\n",
    "with open('{}/global_optimization_results.pkl'.format(save_path), 'wb') as f:\n",
    "    pickle.dump(go_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS AND GET BEST PARAMETERS\n",
    "\n",
    "with open('{}/global_optimization_results.pkl'.format(save_path), 'rb') as f:\n",
    "    go_results = pickle.load(f)\n",
    "\n",
    "new_init_params = go_results.x # use best parameters from global optimization as initial parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrap Optimization\n",
    "\n",
    "We use the best parameters from the global optimization to initialize the optimization process for multiple bootstrap samples of the data. Bootstrap samples are created by sampling with replacement from the original data. We take bootstrap samples of the data and run bootstrap optimization using a local optimization technique (L-BFGS-B) for each bootstrap sample.\n",
    "\n",
    "##### Choosing number of bootstrap samples\n",
    "\n",
    "In order to choose the number of bootstrap samples, we plot the distribution of the parameters for different number of bootstrap samples. We choose the number of bootstrap samples such that the distribution of the parameters is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boots = [20, 50, 100, 200, 500] # number of bootstrap samples\n",
    "\n",
    "def opt_func(x,seed):\n",
    "    \"\"\"\n",
    "    A function to optimize the parameters of the OSFQL-model using a bootstrap sample.\n",
    "\n",
    "    Args:\n",
    "        x (list): list of parameters (alpha_learn, alpha_forget, kappa_omission, kappa_1, kappa_2, ..., kappa_n)\n",
    "        seed (int): seed for random number generator\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    sample = np.random.choice(np.arange(len(c)),len(c),replace=True)\n",
    "    return fit_ql(x,c[sample],r[sample],e[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOTSTRAP OPTIMIZATION USING L-BFGS-B from SciPy\n",
    "\n",
    "for n_boot in n_boots:\n",
    "    print(\"Fitting {} bootstrap samples\".format(n_boot))\n",
    "    res = Parallel(n_jobs=n_jobs)(delayed(opt.minimize)(opt_func,new_init_params,bounds=bounds,args=(i,),method='L-BFGS-B') for i in tqdm(range(n_boot)))\n",
    "    # save as pickle\n",
    "    with open('{}/bootstrap_optimization_results_{}.pkl'.format(save_path,n_boot), 'wb') as f:\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.array([result.x for result in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results using seaborn\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "# plot learning rate\n",
    "sns.distplot(params[:,0], ax=ax[0], kde=False, bins=20)\n",
    "ax[0].set_xlabel('Learning rate')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].set_title('QL learning rate')\n",
    "ax[0].spines['top'].set_visible(False) \n",
    "ax[0].spines['right'].set_visible(False)\n",
    "\n",
    "# plot different reward sensitivities in a boxplot\n",
    "sns.boxplot(data=params[:,1:], ax=ax[1])\n",
    "sns.stripplot(data=params[:,1:], ax=ax[1], color='k', alpha=0.5)\n",
    "# set ticks\n",
    "ax[1].set_xticks(np.arange(len(exp_mapper_inv)), [exp_mapper_inv[i] for i in np.arange(len(exp_mapper_inv))], rotation=90)\n",
    "\n",
    "\n",
    "ax[1].set_xlabel('Reward sensitivity')\n",
    "ax[1].set_ylabel('Value')\n",
    "ax[1].set_title('QL reward sensitivity')\n",
    "ax[1].spines['top'].set_visible(False)\n",
    "ax[1].spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pairwise effect sizes on reward sensitivity\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# calculate effect sizes\n",
    "effect_sizes = np.zeros((len(exp_mapper_inv), len(exp_mapper_inv)))\n",
    "pvals = np.zeros((len(exp_mapper_inv), len(exp_mapper_inv)))\n",
    "for i in range(len(exp_mapper_inv)):\n",
    "    for j in range(len(exp_mapper_inv)):\n",
    "        if i != j:\n",
    "            t, p = ttest_ind(params[:,i+1], params[:,j+1])\n",
    "            # correct for multiple comparisons\n",
    "            p = multipletests([p], method='fdr_bh')[1][0]\n",
    "            effect_sizes[i,j] = (np.mean(params[:,i+1]) - np.mean(params[:,j+1])) / np.std(params[:,i+1] - params[:,j+1])\n",
    "            pvals[i,j] = p\n",
    "\n",
    "\n",
    "# plot effect sizes and p-values\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
    "sns.heatmap(effect_sizes, cmap='RdBu_r', center=0, vmin=-1, vmax=1, ax=ax[0], annot=True, fmt='.2f')\n",
    "ax[0].set_xticklabels(exp_mapper_inv.values(), rotation=90)\n",
    "ax[0].set_yticklabels(exp_mapper_inv.values(), rotation=0)\n",
    "ax[0].set_title('Effect sizes')\n",
    "ax[0].set_xlabel('Experiment')\n",
    "ax[0].set_ylabel('Experiment')\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "sns.heatmap(pvals_corrected, cmap='RdBu_r', center=0.05,ax=ax[1], annot=True, fmt='.2f')\n",
    "ax[1].set_xticklabels(exp_mapper_inv.values(), rotation=90)\n",
    "ax[1].set_yticklabels(exp_mapper_inv.values(), rotation=0)\n",
    "ax[1].set_title('p-values')\n",
    "ax[1].set_xlabel('Experiment')\n",
    "ax[1].set_ylabel('Experiment')\n",
    "ax[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_mapper_inv.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = fit_ql(res.x,c,r,e,return_probs=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[metadata[metadata['Fly Experiment'] == exp].index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cummean(x):\n",
    "    return np.cumsum(x)/np.arange(1,len(x)+1)\n",
    "    \n",
    "current_exp = \"\"\n",
    "for exp in metadata['Fly Experiment'].sort_values().unique():\n",
    "    if exp[:-4].split('_reciprocal')[0] != current_exp:\n",
    "        try:\n",
    "            ax.plot([0, choice_set.shape[1]], [0, choice_set.shape[1]], 'k--')\n",
    "            ax.set_xlabel('Left choices')\n",
    "            ax.set_ylabel('Right choices')\n",
    "            ax.set_xlim([0, choice_set.shape[1]])\n",
    "            ax.set_ylim([0, choice_set.shape[1]])\n",
    "            ax.set_title(current_exp)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            plt.show()\n",
    "        except:\n",
    "            pass\n",
    "        print(exp[:-4].split('_reciprocal')[0])\n",
    "        current_exp = exp[:-4].split('_reciprocal')[0]\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "    choice_set = p[metadata[metadata['Fly Experiment'] == exp].index]\n",
    "    reward_set = reward_data[metadata[metadata['Fly Experiment'] == exp].index]\n",
    "    N_set = choice_set.shape[0]\n",
    "    for i in range(N_set):\n",
    "        reward_rate = np.convolve(reward_set[i,:], np.ones(10)/10, mode='same')\n",
    "        if \"_reciprocal\" in exp:\n",
    "            ax.scatter(np.cumsum(choice_set[i,:]), np.cumsum(1-choice_set[i,:]), c=reward_rate, cmap='jet', s=2)\n",
    "        else:\n",
    "            ax.scatter(np.cumsum(1-choice_set[i,:]), np.cumsum(choice_set[i,:]), c=reward_rate, cmap='jet', s=2)\n",
    "\n",
    "ax.plot([0, choice_set.shape[1]], [0, choice_set.shape[1]], 'k--')\n",
    "ax.set_xlabel('Left choices')\n",
    "ax.set_ylabel('Right choices')\n",
    "ax.set_xlim([0, choice_set.shape[1]])\n",
    "ax.set_ylim([0, choice_set.shape[1]])\n",
    "ax.set_title(current_exp)\n",
    "ax.set_aspect('equal')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('flymazerl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97727a114667ed15117250b5fec8748dc40cf28e874283bb31e511af3ebb2b4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
